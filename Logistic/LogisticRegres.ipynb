{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic回归分类器\n",
    "\n",
    "#### 基于主要思想: 根据现有数据对分类边界线建立回归公式，以此进行分类\n",
    "> 1. 优点：计算代价不高，以与理解和实现\n",
    "> 2. 缺点：容易欠拟合，分类精度不高\n",
    "\n",
    "#### 核心思想：\n",
    "> 1. 首先确定分类器的基础函数，本例子为二类分类，所以采用在大尺度上可近似为Unit Step的Sigmoid函数   \n",
    "Sigmoid: $f(z) = \\frac{1}{1+e^{-z}}$\n",
    "> 2. 回归: 对数据进行直线线性拟合的过程称为回归，即找到一条直线，使得数据点到直线的距离最小。在此例中，我们假设Sigmoid函数的输入为$z$，那么有:   \n",
    "$z = \\sum_{i=1}^{n}w_ix_i$，其中$w_i$为回归系数，$x_i$为输入数据，将$w_i$写作向量$\\mathbf{w}$，则这个向量就是训练数据集的目标最佳回归系数向量。\n",
    "> 3. 梯度上升法(最优化算法)   \n",
    "假设函数$f(x,y)$，记其梯度为$\\nabla f(x,y)$，该函数连续且处处可微，则其梯度为:   \n",
    "$\\nabla f(x,y) = \\begin{pmatrix} \\frac{\\partial f}{\\partial x} & \\frac{\\partial f}{\\partial y} \\end{pmatrix}^T$   \n",
    "它表示要沿x轴方向移动$\\frac{\\partial f}{\\partial x}$，沿y轴方向移动$\\frac{\\partial f}{\\partial y}$\n",
    "梯度上升算法到达每个点都会重新估计移动的方向，循环迭代直到满足停止条件，梯度算子总能保证选取的是最佳移动方向，而这个梯度算子在算法中只起到指引作用，还需要乘以步长值$\\alpha$得到最终的移动量，那么梯度上升算法的迭代公式如下：   \n",
    "$w:=w+\\alpha\\nabla_w f(w)$   \n",
    "同理，对于梯度下降算法，其迭代公式为: $w:=w-\\alpha\\nabla_w f(w)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def sigmoid(inX):\n",
    "    \"\"\"\n",
    "    sigmoid函数\n",
    "    \"\"\"\n",
    "    return 1.0 / (1 + exp(-inX))\n",
    "\n",
    "def gradAscent(dataMatIn, classLabels):\n",
    "    \"\"\"\n",
    "    梯度上升算法\n",
    "    :param  dataMatIn   输入的数据矩阵\n",
    "    :param  classLabels 类别标签\n",
    "    :return 权重\n",
    "    \"\"\"\n",
    "    # 将输入的数据矩阵转换为numpy的mat类型\n",
    "    dataMatrix = matrix(dataMatIn)\n",
    "    # 将类别标签转换为列向量，以方便矩阵运算\n",
    "    labelMat = matrix(classLabels).transpose()\n",
    "    # 获取输入矩阵的形状，m为样本数(行数)，n为特征数(列数)\n",
    "    m, n = shape(dataMatrix)\n",
    "    # 设置步长\n",
    "    alpha = 0.001\n",
    "    # 设置最大迭代次数\n",
    "    maxCycles = 500\n",
    "    # 初始化权重矩阵为1矩阵，形状为(特征数，1)\n",
    "    weights = ones((n, 1))\n",
    "    for k in range(maxCycles):\n",
    "        # 计算当前权重下的预测值h，这里dataMatrix * weights返回的是一个维数为100的列向量，所以h也是一个列向量\n",
    "        h = sigmoid(dataMatrix * weights)\n",
    "        # 计算预测值与实际标签的误差，返回的是列表(行向量)\n",
    "        error = (labelMat - h)\n",
    "        # 更新权重列向量\n",
    "        weights = weights + alpha * dataMatrix.transpose() * error\n",
    "    # 返回最终的权重矩阵\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度上升"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
