{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb62786867a52ff7",
   "metadata": {},
   "source": [
    "# ___**决策树**___\n",
    "### 基本原理\n",
    "> - 从前面的kNN算法的基本原理和实现来看，kNN算法的最大/最显著的缺点是无法给出数据的内在含义，而这也是决策树的主要优势，其数据形式非常易于理解。\n",
    "> - 决策树的一个重要任务是为了理解数据中所蕴含的知识信息，因此决策树可以使用不熟悉的数据集合，并从中提取出一系列规则，而这些机器根据数据集创建规则的过程，就是机器学习的过程\n",
    "> - 决策树也是一个典型的监督学习算法\n",
    "> - 简要来说，构建决策树的过程就是将现有数据不断划分，直到每个划分出的数据子集中的数据归于的类型相同。\n",
    "### 数据划分、信息增益和熵\n",
    "> - 划分数据集的大原则是: 将无序的数据变得更加有序\n",
    "> - 在划分数据增益前后信息发生的变化称为信息增益，以某个特征值划分数据后获得信息增益最大时，该特征值就是最好的选择\n",
    "> - 集合信息的度量方式称为香农熵或者熵(采用)，熵定义为信息的期望值\n",
    "- 计算给定数据集的熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2886520c01c71b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "def clacShannonEnt(dataSet):\n",
    "    \"\"\"\n",
    "    计算给定数据集的熵(香农熵)\n",
    "    :param dataSet: \n",
    "    :return: 香农熵\n",
    "    \"\"\"\n",
    "    # 获取数据集数据的数量\n",
    "    numEntries = len(dataSet)\n",
    "    # 创建空的标签字典，用于存储标签及该标签的出现次数\n",
    "    labelCounts = {}\n",
    "    # 遍历数据集中的每一个数据样本\n",
    "    for featVec in dataSet:\n",
    "        # 获得当前数据样本的标签\n",
    "        currentLabel = featVec[-1]\n",
    "        # 如果这个标签在标签字典中不存在\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            # 则初始化这个标签在字典中的值\n",
    "            labelCounts[currentLabel] = 0\n",
    "        # 更新标签出现的次数\n",
    "        labelCounts[currentLabel] += 1\n",
    "    shannonEnt = 0.0\n",
    "    for key in labelCounts:\n",
    "        # 计算在数据集中每个标签出现的概率，就是该标签数量除以样本总数量\n",
    "        prob = float(labelCounts[key]) / numEntries\n",
    "        # 香农熵的累加，原始公式为-Σ(总样本数) 该标签出现的概率*该概率的底数为2的对数\n",
    "        shannonEnt -= prob * log(prob, 2)\n",
    "    # 熵越高，代表着混合的数据越多，数据越难以区分，可以通过增加额外的标签来进一步区分数据以降低熵\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1194167",
   "metadata": {},
   "source": [
    "# ___**Attention: Numpy库中的log函数和math库的log有冲突，如果```from math import log```，则不能用```from numpy import *```**___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875cdf122be9faa",
   "metadata": {},
   "source": [
    "### ___**此外，另一个度量信息无序程度的方法是基尼不纯度(Gini impurity)，简单来说就是从一个数据集中随机选取子项，度量其被错误分类到其他分组里的概率**___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618bf09af91f348",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7074860829570d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T13:25:36.525199Z",
     "start_time": "2024-10-26T13:25:36.504107Z"
    }
   },
   "outputs": [],
   "source": [
    "def splitDataSet(dataSet, axis, value):\n",
    "    \"\"\"\n",
    "    划分数据集\n",
    "    :param dataSet: 待划分的数据集\n",
    "    :param axis:    划分数据集的特征\n",
    "    :param value:   需要返回的特征的值\n",
    "    :return:        划分后的数据集\n",
    "    \"\"\"\n",
    "    # 创建空列表用于存储划分后的数据集\n",
    "    retDataSet = []\n",
    "    for featVec in dataSet:\n",
    "        # 如果该样本的特征值与需要返回的特征值相同，则将该样本抽取出来划分到新的数据集中\n",
    "        if featVec[axis] == value:\n",
    "            # 切片获取从列表开头到axis-1的元素\n",
    "            reducedFeatVec = featVec[:axis]\n",
    "            # 切片追加从axis+1到列表末尾的元素，现在列表reducedFeatVec包含了除axis索引以外所有的元素\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            # 追加到划分后的数据集中\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b6c4c3c3bf2d49",
   "metadata": {},
   "source": [
    "### 选择最好的数据集划分方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebad08eb697591b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from 决策树.Decision_Tree import calcShannonEnt, majorityCnt\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):\n",
    "    \"\"\"\n",
    "    选择最好的数据集划分方式\n",
    "    :param dataSet: 原始数据集\n",
    "    :return:        最佳划分特征\n",
    "    \"\"\"\n",
    "    # 获取数据集的特征数量\n",
    "    numFeatures = len(dataSet[0]) - 1\n",
    "    # 获取原始数据集的熵\n",
    "    baseEntropy = calcShannonEnt(dataSet)\n",
    "    # 声明最佳信息增益和最佳特征索引\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeature = -1\n",
    "    # 遍历所有特征\n",
    "    for i in range(numFeatures):\n",
    "        # 使用条件for获取一个样本的所有特征值列表\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        # 将特征值列表转换为集合，用于去除重复特征值\n",
    "        uniqueVals = set(featList)\n",
    "        # 声明划分数据后的熵\n",
    "        newEntropy = 0.0\n",
    "        # 遍历特征集合中的每个特征值\n",
    "        for value in uniqueVals:\n",
    "            # 划分数据集\n",
    "            subDataSet = splitDataSet(dataSet, i, value)\n",
    "            # 计算划分后的数据集的熵\n",
    "            prob = len(subDataSet) / float(len(dataSet))\n",
    "            newEntropy += prob * calcShannonEnt(subDataSet)\n",
    "        # 计算信息增益\n",
    "        infoGain = baseEntropy - newEntropy\n",
    "        # 如果信息增益大于最佳信息增益，则更新最佳信息增益和最佳特征\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d73a472422499d",
   "metadata": {},
   "source": [
    "# 递归构建决策树\n",
    "- #### 递归的终止条件: 程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2397dbcb1ddc81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    递归构建决策树\n",
    "    :param dataSet: 数据集\n",
    "    :param labels:  标签集，算法本身不需要这个变量，但是为了给出数据明确的含义，需要给出标签机\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # 获得数据集的所有特征标签\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "    # 停止条件1: 如果数据集中所有实例属于同一类别，直接返回该类别\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    # 停止条件2: 如果使用完了所有特征仍不能将数据集划分成仅包含唯一类别的分组，则返回出现次数最多的类别\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    # 获得当前数据集的最佳划分特征\n",
    "    bestFeat = chooseBestFeatureToSplit(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    # 创建空嵌套字典用于存储子树\n",
    "    myTree = {bestFeatLabel: {}}\n",
    "    # 去除标签列表中的最佳特征\n",
    "    del labels[bestFeat]\n",
    "    # 获得数据集的所有特征标签\n",
    "    featValues = [example[bestFeat] for example in dataSet]\n",
    "    # 去除重复的特征值\n",
    "    uniqueVals = set(featValues)\n",
    "    # 遍历特征值集合中的每个特征\n",
    "    for value in uniqueVals:\n",
    "        # 获得标签集的所有标签\n",
    "        subLabels = labels[:]\n",
    "        # 递归构建子树: 最优划分特征下的每个值都对应一个子树\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n",
    "    # 返回子树\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f9c02a",
   "metadata": {},
   "source": [
    "# 计算决策树的叶子数量和深度\n",
    "- #### 其实是数据结构的基本算法，都是采用递归计算的\n",
    "- ##### 但是需要注意的点是，py3中字典的.keys()方法返回的是一个视图对象，而在py2中是列表，所以需要使用list()函数转换一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e460d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLeafNum(myTree):\n",
    "    \"\"\"\n",
    "    获取决策树的叶子节点数\n",
    "    :param myTree 决策树\n",
    "    \"\"\"\n",
    "    leafNum = 0\n",
    "    firststr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firststr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            leafNum += getLeafNum(secondDict[key])\n",
    "        else:\n",
    "            leafNum += 1\n",
    "    return leafNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e54d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTreeDepth(myTree):\n",
    "    \"\"\"\n",
    "    计算决策树的深度\n",
    "    : param myTree: 决策树\n",
    "    : return: 决策树的深度\n",
    "    \"\"\"\n",
    "    treeDepth = 0\n",
    "    firststr = list(myTree.keys())[0]\n",
    "    secondDict = myTree[firststr]\n",
    "    for key in secondDict.keys():\n",
    "        if type(secondDict[key]).__name__ == 'dict':\n",
    "            thisDepth = 1 + getTreeDepth(secondDict[key])\n",
    "        else:\n",
    "            thisDepth = 1\n",
    "        if thisDepth > treeDepth:\n",
    "            treeDepth = thisDepth\n",
    "    return treeDepth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d4ee5",
   "metadata": {},
   "source": [
    "# 创建基于决策树算法的分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(inputTree, featLabels, testVec):\n",
    "    \"\"\"\n",
    "    调用决策树算法进行分类，实际上就是测试向量遍历决策树的过程\n",
    "    : inputTree   决策树\n",
    "    : featLabels  特征标签列表\n",
    "    : testVec     测试向量\n",
    "    : return      分类结果\n",
    "    \"\"\"\n",
    "    # 获得当前所在树的第一个键\n",
    "    firstStr = list(inputTree.keys())[0]\n",
    "    # 获得当前键对应的值，也就是子树\n",
    "    secondDict = inputTree[firstStr]\n",
    "    # 获得测试向量的第一个特征的索引\n",
    "    featIndex = featLabels.index(firstStr)\n",
    "    # 遍历子树的所有键\n",
    "    for key in secondDict.keys():\n",
    "        # 如果先前获得的特征索引在测试向量的元素与当前键相同，且当前键的类型是字典，则递归调用分类函数，否则返回当前键对应的值\n",
    "        if testVec[featIndex] == key:\n",
    "            if type(secondDict[key]).__name__ == 'dict':\n",
    "                classLabel = classify(secondDict[key], featLabels, testVec)\n",
    "            else:\n",
    "                classLabel = secondDict[key]\n",
    "    return classLabel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45878e9",
   "metadata": {},
   "source": [
    "# 使用pickle模块保存和加载决策树模型，这样就可以避免每次分类都需要重新构建决策树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5203131",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 本质上使用pickle模块存储决策树是利用了python的序列化机制，将决策树对象序列化为一个文件，可以将其保存到本地，或者通过网络传输到其他地方，换句话说，python支持将复杂的对象序列化为文件，并在需要时反序列化为原来的对象。\n",
    "\n",
    "def storeTree(inputTree, filename):\n",
    "    fw = open(filename, 'w')\n",
    "    pickle.dump(inputTree, fw)\n",
    "    fw.close()\n",
    "\n",
    "def grabTree(filename):\n",
    "    fr = open(filename)\n",
    "    return pickle.load(fr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
