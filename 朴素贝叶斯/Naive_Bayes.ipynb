{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___**朴素贝叶斯算法**___\n",
    "- #### 优点：在数据量较少的情况下依旧有效，可以处理多类别问题\n",
    "- #### 缺点：对输入数据的准备方式较为敏感\n",
    "- **朴素贝叶斯分类器的重要假设**：\n",
    "> 1. 数据样本的每一个特征之间互相独立，即一个特征出现的可能性与它和其他特征相邻无关\n",
    "> 2. 每个特征同等重要\n",
    "#### 朴素贝叶斯分类器适用于标称性数据，即特征取值需离散，对于输入数据而言最好的方式是将其离散为数字数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputData):\n",
    "    \"\"\"\n",
    "    以原书的实现为例,将输入的文档转换为词向量,也就是一组以0/1表示的词频向量。\n",
    "    :param vocabList 词汇表\n",
    "    :param inputData 输入文档\n",
    "    :return 词频向量\n",
    "    \"\"\"\n",
    "    # 创建维数与输入的词汇表长度相同的0向量\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    # 遍历输入文档中的每个词，如果词在词汇表中，则将词频设置为1，表示出现过\n",
    "    for word in inputData:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else:\n",
    "            print(f\"the word: {word} is not in my vocabulary\")\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 朴素贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def trainNB0(trainMatrix, trainCategory):\n",
    "    \"\"\"\n",
    "    训练朴素贝叶斯分类器\n",
    "    :param trainMatrix   文档矩阵,行向量表示文档,传递的参数应该为一个词频的二值矩阵,也就是调用上述的setOfWords2Vec函数得到的词频矩阵\n",
    "    :param trainCategory 类别标签向量，维数同文档矩阵函数应当相等\n",
    "    :return 分类器模型\n",
    "    \"\"\"\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    # 这里假设文档矩阵中的文档的词数相同\n",
    "    numWords = len(trainMatrix[0])\n",
    "    # 计算先验概率，实际上就是文档矩阵中所有的带有侮辱性词汇的文档数除以总文档数，在此例子中，标签向量的元素值是二值化的\n",
    "    pAbusive = sum(trainCategory) / float(numTrainDocs)\n",
    "    # 初始化条件概率，分子numerator和分母denominator\n",
    "    p0Num = ones(numWords)\n",
    "    p1Num = ones(numWords)\n",
    "    p0Denom = 2.0\n",
    "    p1Denom = 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        # 倘若当前的文档的标签为1，则将该文档的词频率向量加到p1Num中，否则加到p0Num中，并将词频率总和加到相应的分母中\n",
    "        if trainCategory[i] == 1:\n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    # 计算条件概率，此处采用了对数运算是为了防止过小的数的连乘导致的向下溢出，对比f(x)和ln(f(x))，他们的增长性和极点取值几乎相同，可以视作结果相同\n",
    "    p1Vect = log(p1Num / p1Denom)\n",
    "    p0Vect = log(p0Num / p0Denom)\n",
    "    return p0Vect, p1Vect, pAbusive"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
